{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab14_exercises",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#High-dimensional regression"
      ],
      "metadata": {
        "id": "VU95foPgSR7C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import the necessary libraries**"
      ],
      "metadata": {
        "id": "UaQPVA_VSZo4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1X5-IyjYbGFK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt \n",
        "from sklearn.linear_model import Ridge, Lasso, ElasticNet, LinearRegression\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.pipeline import Pipeline \n",
        "from sklearn.decomposition import PCA\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Use the code snippet below to generate some data points**"
      ],
      "metadata": {
        "id": "kWYl55GYSgyc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y, coef = datasets.make_regression(n_samples=100,#number of samples\n",
        "                                      n_features=5,#number of features\n",
        "                                      n_informative=5,#number of useful features \n",
        "                                      noise=10,#standard deviation of the guassian noise\n",
        "                                      coef=True,#true coefficient used to generated the data\n",
        "                                      random_state=0) #set for same data points for each run\n"
      ],
      "metadata": {
        "id": "bUOiWN0wbQl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1) For $\\lambda$ varying between 0 and 1000 by steps of 1, fit a ridge regression to the data and plot the evolution of the coefficients as a function of $\\lambda$. What do you observe ? Redo the same operation for a Lasso regression.**"
      ],
      "metadata": {
        "id": "JTVEaod7Sssr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2) Reuse the code snippet above to generate 100 samples with 90 features, with only 2 being informative on the response $y$. Split your dataset on a train and a test split using a 80/20 partition. For increasing values of $\\lambda$, fit a ridge regression model on the training data and plot its MSE as a function of $\\lambda$. What do you observe ? Do the same for a Lasso regression.** "
      ],
      "metadata": {
        "id": "aOvMOvEVUQ4S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3) Reuse the code above to generate 1000 samples, with 90 features amongst which 2 are actually useful. Split your data into a train and test set following a 80/20 partition. For a Lasso and a Ridge regression, perform a cross-validation on the training data to select the best $\\lambda$. Also perform a cross-validation on a Principal Component Regression to select the best number of components.**\n",
        "\n",
        "**Once you found the best hyper-parameter for each model, refit them on the training data and report their MSE on the test set. Which model achieves the lowest error ?**"
      ],
      "metadata": {
        "id": "UHd9xeGjZJ5g"
      }
    }
  ]
}